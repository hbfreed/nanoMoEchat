"""
GPT model (rewrite, a lot simpler)
Notable features:
- rotary embeddings (and no positional embeddings)
- QK norm
- untied weights for token embedding and lm_head
- relu^2 activation in MLP
- norm after token embedding
- no learnable params in rmsnorm
- no bias in linear layers
- Multi-Query Attention (MQA) support for more efficient inference
"""

import math
from dataclasses import dataclass, field
from functools import partial

import stk
import stk.ops
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from megablocks import ops
from megablocks.layers.relu_squared import relu_squared

from nanochat.adamw import DistAdamW
from nanochat.common import get_dist_info, print0
from nanochat.muon import DistMuon, Muon
from nanochat.topology_var import topology_var

@dataclass
class GPTConfig:
    sequence_len: int = 1024
    vocab_size: int = 50304
    n_layer: int = 12
    n_head: int = 6  # number of query heads
    n_kv_head: int = 6  # number of key/value heads (MQA)
    n_embd: int = 768
    use_moe: bool = True
    use_triton_moe: bool = True  # Use Triton kernels instead of stk (requires triton-variable-moe)
    expert_sizes: list = field(
        default_factory=lambda: [(64, 256)]
    )  # 64 fine-grained experts
    num_active_experts: int = 8
    norm_topk_prob: bool = True
    block_size: int = 128  # Triton kernel tile size for MoE
    load_balance_loss_weight: float = 0.08
    router_z_loss_weight: float = 0.001
    compute_loss_weight: float = 0.004


def norm(x):
    # Purely functional rmsnorm with no learnable params
    return F.rms_norm(x, (x.size(-1),))


def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4  # multihead attention
    d = x.shape[3] // 2
    x1, x2 = x[..., :d], x[..., d:]  # split up last time into two halves
    y1 = x1 * cos + x2 * sin  # rotate pairs of dims
    y2 = x1 * (-sin) + x2 * cos
    out = torch.cat([y1, y2], 3)  # re-assemble
    out = out.to(x.dtype)  # ensure input/output dtypes match
    return out


class CausalSelfAttention(nn.Module):
    def __init__(self, config, layer_idx):
        super().__init__()
        self.layer_idx = layer_idx
        self.n_head = config.n_head
        self.n_kv_head = config.n_kv_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        assert self.n_kv_head <= self.n_head and self.n_head % self.n_kv_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)

    def forward(self, x, cos_sin, kv_cache):
        B, T, C = x.size()

        # Project the input to get queries, keys, and values
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_kv_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_kv_head, self.head_dim)

        # Apply Rotary Embeddings to queries and keys to get relative positional encoding
        cos, sin = cos_sin
        q, k = (
            apply_rotary_emb(q, cos, sin),
            apply_rotary_emb(k, cos, sin),
        )  # QK rotary embedding
        q, k = norm(q), norm(k)  # QK norm
        q, k, v = (
            q.transpose(1, 2),
            k.transpose(1, 2),
            v.transpose(1, 2),
        )  # make head be batch dim, i.e. (B, T, H, D) -> (B, H, T, D)

        # Apply KV cache: insert current k,v into cache, get the full view so far
        if kv_cache is not None:
            k, v = kv_cache.insert_kv(self.layer_idx, k, v)
        Tq = q.size(2)  # number of queries in this forward pass
        Tk = k.size(
            2
        )  # number of keys/values in total (in the cache + current forward pass)

        # Attention: queries attend to keys/values autoregressively. A few cases to handle:
        enable_gqa = (
            self.n_head != self.n_kv_head
        )  # Group Query Attention (GQA): duplicate key/value heads to match query heads if desired
        if kv_cache is None or Tq == Tk:
            # During training (no KV cache), attend as usual with causal attention
            # And even if there is KV cache, we can still use this simple version when Tq == Tk
            y = F.scaled_dot_product_attention(
                q, k, v, is_causal=True, enable_gqa=enable_gqa
            )
        elif Tq == 1:
            # During inference but with a single query in this forward pass:
            # The query has to attend to all the keys/values in the cache
            y = F.scaled_dot_product_attention(
                q, k, v, is_causal=False, enable_gqa=enable_gqa
            )
        else:
            # During inference AND we have a chunk of queries in this forward pass:
            # First, each query attends to all the cached keys/values (i.e. full prefix)
            attn_mask = torch.zeros(
                (Tq, Tk), dtype=torch.bool, device=q.device
            )  # True = keep, False = mask
            prefix_len = Tk - Tq
            if prefix_len > 0:  # can't be negative but could be zero
                attn_mask[:, :prefix_len] = True
            # Then, causal attention within this chunk
            attn_mask[:, prefix_len:] = torch.tril(
                torch.ones((Tq, Tq), dtype=torch.bool, device=q.device)
            )
            y = F.scaled_dot_product_attention(
                q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa
            )

        # Re-assemble the heads side by side and project back to residual stream
        y = y.transpose(1, 2).contiguous().view(B, T, -1)
        y = self.c_proj(y)
        return y


class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square()
        x = self.c_proj(x)
        return x, None, None


class MoEMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.num_experts = sum(count for count, _ in config.expert_sizes)
        self.num_active_experts = config.num_active_experts
        self.norm_topk_prob = config.norm_topk_prob
        self.block_size = 128  # always will be 128.

        # expert_widths: FFN width for each expert, expanded from config.expert_sizes tuples
        # e.g. config.expert_sizes=[(2, 1024), (1, 512)] -> expert_widths=[1024, 1024, 512]
        self.expert_widths = []
        self.expert_offsets = [0]
        for count, size in config.expert_sizes:
            assert size % 128 == 0, "expert sizes must be divisible by 128"
            for _ in range(count):
                self.expert_widths.append(size)
                self.expert_offsets.append(self.expert_offsets[-1] + size)
        self.total_expert_width = self.expert_offsets[-1]

        # compute normalized expert widths for aux losses
        mean_expert_width = sum(self.expert_widths) / self.num_experts
        self.register_buffer(
            "expert_widths_normalized",
            torch.tensor(
                [w / mean_expert_width for w in self.expert_widths], dtype=torch.float32
            ),
            persistent=False,
        )

        self.router = nn.Linear(config.n_embd, self.num_experts, bias=False)

        self.w1 = nn.Parameter(torch.empty(config.n_embd, self.total_expert_width))
        self.w2 = nn.Parameter(torch.empty(self.total_expert_width, config.n_embd))

        nn.init.trunc_normal_(self.w1, mean=0.0, std=0.02, a=-0.06, b=0.06)
        nn.init.trunc_normal_(self.w2, mean=0.0, std=0.02, a=-0.06, b=0.06)

        # need this for megablocks ops
        self.sort_end_bit = max(int(math.ceil(math.log2(self.num_experts))), 1)

        self.transpose_sort_end_bit = max(
            int(math.ceil(math.log2(self.num_experts))), 1
        )

        # Register buffers for efficient CUDA kernel access
        self.register_buffer(
            "expert_size_blocks",
            torch.tensor(
                [s // self.block_size for s in self.expert_widths], dtype=torch.int32
            ),
            persistent=False,
        )
        self.register_buffer(
            "expert_block_offsets",
            torch.tensor(
                [o // self.block_size for o in self.expert_offsets], dtype=torch.int32
            ),
            persistent=False,
        )

        # Precompute tensors for vectorized load balance loss computation
        expert_to_group = []
        group_sizes = []
        valid_group_idx = 0
        for count, size in config.expert_sizes:
            for _ in range(count):
                if count > 1:
                    expert_to_group.append(valid_group_idx)
                else:
                    expert_to_group.append(-1)
            if count > 1:
                group_sizes.append(count)
                valid_group_idx += 1

        self._num_valid_groups = len(group_sizes)
        self._all_experts_valid = all(g >= 0 for g in expert_to_group)

        if self._num_valid_groups > 0:
            group_membership = torch.zeros(self.num_experts, self._num_valid_groups)
            for i, g in enumerate(expert_to_group):
                if g >= 0:
                    group_membership[i, g] = 1.0

            self.register_buffer("group_membership", group_membership, persistent=False)
            self.register_buffer(
                "group_sizes",
                torch.tensor(group_sizes, dtype=torch.float32),
                persistent=False,
            )
            valid_indices = [i for i, g in enumerate(expert_to_group) if g >= 0]
            self.register_buffer(
                "valid_expert_indices",
                torch.tensor(valid_indices, dtype=torch.long),
                persistent=False,
            )
        else:
            self.register_buffer(
                "group_membership", torch.empty(0), persistent=False
            )
            self.register_buffer(
                "group_sizes", torch.tensor([1.0]), persistent=False
            )
            self.register_buffer(
                "valid_expert_indices", torch.empty(0, dtype=torch.long), persistent=False
            )


    # Disable torch.compile tracing for MoE - triton kernels (stk.ops.row_indices etc)
    # can't handle FakeTensors used during compile tracing
    @torch.compiler.disable
    def forward(self, x):
        batch_size, seq_len, n_embd = x.shape

        x_flat = rearrange(
            x, "batch_size seq_len n_embd -> (batch_size seq_len) n_embd "
        )

        router_logits = self.router(x_flat)

        # router_probs = F.softmax(
        #     router_logits, dim=-1, dtype=torch.float32
        # )
        router_probs = F.sigmoid(
            router_logits.to(torch.float32)
        )  # seeing if we really need to cast to float32, guessing probably

        top_k_weights, selected_experts = torch.topk(
            router_probs, self.num_active_experts, dim=-1
        )

        top_k_weights = top_k_weights / (
            top_k_weights.sum(dim=-1, keepdim=True) + 1e-20
        )  # epsilon so we don't divide by 0

        top_k_weights = top_k_weights.to(x.dtype)

        top_k_weights_flat = rearrange(top_k_weights, "... -> (...)")
        selected_experts_flat = rearrange(selected_experts, "... -> (...)")

        bin_ids, indices, tokens_per_expert = self._sort_tokens_by_expert(
            selected_experts_flat
        )

        # Compute bins for gather/scatter
        bins = ops.inclusive_cumsum(tokens_per_expert, 0).contiguous()

        # Build topology dynamically each forward (like dMoE)
        padded_bins, topology = self._create_topology(x_flat, tokens_per_expert)

        x_permuted = ops.padded_gather(
            x_flat, indices, bin_ids, bins, padded_bins, self.num_active_experts
        )
        x_permuted = stk.ops.sdd(x_permuted, self.w1, topology)
        x_permuted = relu_squared(x_permuted)
        x_permuted = stk.ops.dsd(x_permuted, self.w2)
        x_permuted = ops.padded_scatter(
            x_permuted,
            indices,
            bin_ids,
            top_k_weights_flat,
            bins,
            padded_bins,
            self.num_active_experts,
        )
        output = rearrange(
            x_permuted,
            "(batch_size seq_len) n_embd -> batch_size seq_len n_embd",
            batch_size=batch_size,
            seq_len=seq_len,
        )

        router_z_loss = torch.logsumexp(router_logits, dim=-1).pow(2).mean()

        # Reuse histogram from routing instead of slow scatter_add_
        f_i = (tokens_per_expert.float() / tokens_per_expert.sum()).to(x.dtype)
        load_balance_loss = self._compute_load_balance_loss(
            router_probs, selected_experts_flat, f_i
        )
        router_probs_flat = rearrange(
            router_probs,
            "(batch_size seq_len) n_embd -> (batch_size seq_len) n_embd",
            batch_size=batch_size,
            seq_len=seq_len,
        )
        compute_loss = (
            router_probs_flat
            @ self.expert_widths_normalized.to(router_probs_flat.dtype)
        ).mean()

        aux_loss = {
            "router_z_loss": router_z_loss,
            "load_balance_loss": load_balance_loss,
            "compute_loss": compute_loss,
        }

        return output, aux_loss, f_i

    def _sort_tokens_by_expert(self, selected_experts_flat):
        """Group token assignments by expert id."""

        bin_ids, indices = ops.sort(selected_experts_flat, self.sort_end_bit)
        tokens_per_expert = ops.histogram(selected_experts_flat, self.num_experts)

        return bin_ids, indices, tokens_per_expert

    def _create_topology(self, x, tokens_per_expert):
        padded_tokens_per_expert = ops.round_up(tokens_per_expert, self.block_size)
        padded_bins = ops.inclusive_cumsum(padded_tokens_per_expert, 0)
        padded_bins = padded_bins.contiguous()

        padded_tokens = padded_bins[-1].clamp_min(self.block_size)

        block_rows = padded_tokens // self.block_size

        # Use variable-size topology with per-expert block counts
        column_indices = topology_var(
            padded_bins,
            self.expert_size_blocks,  # Per-expert block counts
            self.expert_block_offsets,  # Cumulative block offsets
            self.block_size,
            block_rows,
        )

        # Compute all expert token blocks at once
        expert_token_blocks = padded_tokens_per_expert // self.block_size

        # Repeat each expert's size by how many token blocks it handles
        repeated_sizes = torch.repeat_interleave(
            self.expert_size_blocks, expert_token_blocks
        )

        # Cumulative sum gives you offsets
        offsets = torch.cat([repeated_sizes.new_zeros(1), repeated_sizes.cumsum(0)])

        column_indices = column_indices.to(torch.int32)
        offsets = offsets.to(torch.int32)

        shape = (padded_tokens, self.total_expert_width)

        num_blocks = column_indices.numel()
        data_placeholder = torch.empty(
            num_blocks,
            self.block_size,
            self.block_size,
            dtype=x.dtype,
            device="meta",
        )

        row_indices = stk.ops.row_indices(
            shape, data_placeholder, offsets, column_indices
        )
        row_indices = row_indices.to(torch.int32)

        column_indices_t, offsets_t, block_offsets_t = self._sparse_transpose(
            shape, row_indices, column_indices
        )
        column_indices_t = column_indices_t.to(torch.int32)
        offsets_t = offsets_t.to(torch.int32)
        block_offsets_t = block_offsets_t.to(torch.int32)

        topology = stk.Matrix(
            shape,
            data_placeholder,
            row_indices,
            column_indices,
            offsets,
            column_indices_t,
            offsets_t,
            block_offsets_t,
        )

        return padded_bins, topology

    def _sparse_transpose(self, size, row_indices, column_indices):
        # Use total_expert_width instead of d_ffn * num_experts
        block_columns = self.total_expert_width // self.block_size

        _, gather_indices = ops.sort(
            column_indices.int(),
            self.transpose_sort_end_bit,
        )

        column_indices_t = row_indices.gather(0, gather_indices.long())
        block_offsets_t = gather_indices.int()

        zero = torch.zeros((1,), dtype=torch.int32, device=row_indices.device)
        nnz_per_column = ops.histogram(column_indices, block_columns)
        nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)
        if nnz_per_column.dim() == 0:
            # This addresses an edge case when ffn_hidden_size is equal to self.block_size.
            nnz_per_column = nnz_per_column.unsqueeze(0)
        offsets_t = torch.cat([zero, nnz_per_column])
        return column_indices_t, offsets_t, block_offsets_t

    def _gather_tokens(
        self, x, indices, bin_ids, tokens_per_expert, padded_bins, bins=None
    ):
        if bins is None:
            bins = ops.inclusive_cumsum(tokens_per_expert, 0)
            bins = bins.contiguous()

        return ops.padded_gather(
            x, indices, bin_ids, bins, padded_bins, self.num_active_experts
        )

    def _scatter_tokens(
        self, x, indices, bin_ids, weights, tokens_per_expert, padded_bins, bins=None
    ):
        """Un-permute tokens and apply expert weights."""
        if bins is None:
            bins = ops.inclusive_cumsum(tokens_per_expert, 0)
            bins = bins.contiguous()

        return ops.padded_scatter(
            x, indices, bin_ids, weights, bins, padded_bins, self.num_active_experts
        )

    def _compute_load_balance_loss(self, router_probs, experts_flat, f_i):
        """Compute load balance loss within expert groups (vectorized)."""
        p_i = router_probs.mean(dim=0)

        if len(set(self.expert_widths)) == 1:
            # for uniform expert sizes, just compute regular lbl loss
            return self.num_experts * (f_i.float() @ p_i.float())

        if self._num_valid_groups == 0:
            return f_i.float() @ p_i.float()

        fi_pi = f_i.float() * p_i.float()

        membership = self.group_membership.to(fi_pi.dtype)
        if self._all_experts_valid:
            group_sums = fi_pi @ membership
        else:
            fi_pi_valid = fi_pi.index_select(0, self.valid_expert_indices)
            membership_valid = membership.index_select(0, self.valid_expert_indices)
            group_sums = fi_pi_valid @ membership_valid

        group_losses = self.group_sizes.to(fi_pi.dtype) * group_sums
        return group_losses.mean()


class Block(nn.Module):
    def __init__(self, config, layer_idx):
        super().__init__()
        self.attn = CausalSelfAttention(config, layer_idx)
        if config.use_moe:
            if config.use_triton_moe:
                from triton_moe import TritonMoEMLP, TritonMoEConfig

                tri_config = TritonMoEConfig(
                    n_embd=config.n_embd,
                    expert_sizes=config.expert_sizes,
                    num_active_experts=config.num_active_experts,
                    norm_topk_prob=config.norm_topk_prob,
                    block_size=config.block_size,
                )
                self.mlp = TritonMoEMLP(tri_config)
            else:
                self.mlp = MoEMLP(config)
        else:
            self.mlp = MLP(config)

    def forward(self, x, cos_sin, kv_cache):
        x = x + self.attn(norm(x), cos_sin, kv_cache)
        mlp_x, aux_loss, f_i = self.mlp(norm(x))
        x = x + mlp_x
        return x, aux_loss, f_i


class GPT(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.transformer = nn.ModuleDict(
            {
                "wte": nn.Embedding(config.vocab_size, config.n_embd),
                "h": nn.ModuleList(
                    [Block(config, layer_idx) for layer_idx in range(config.n_layer)]
                ),
            }
        )
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # To support meta device initialization, we init the rotary embeddings here, but it's fake
        # As for rotary_seq_len, these rotary embeddings are pretty small/cheap in memory,
        # so let's just over-compute them, but assert fail if we ever reach that amount.
        # In the future we can dynamically grow the cache, for now it's fine.
        self.rotary_seq_len = (
            config.sequence_len * 10
        )  # 10X over-compute should be enough, TODO make nicer?
        head_dim = config.n_embd // config.n_head
        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)
        self.register_buffer(
            "cos", cos, persistent=False
        )  # persistent=False means it's not saved to the checkpoint
        self.register_buffer("sin", sin, persistent=False)

    def init_weights(self):
        self.apply(self._init_weights)
        # zero out classifier weights
        torch.nn.init.zeros_(self.lm_head.weight)
        # zero out c_proj weights in all blocks (MoE uses trunc_normal init for w2 instead)
        for block in self.transformer.h:
            if hasattr(block.mlp, "c_proj"):
                torch.nn.init.zeros_(block.mlp.c_proj.weight)
            torch.nn.init.zeros_(block.attn.c_proj.weight)
        # init the rotary embeddings
        head_dim = self.config.n_embd // self.config.n_head
        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)
        self.cos, self.sin = cos, sin
        # Cast the embeddings from fp32 to bf16: optim can tolerate it and it saves memory: both in the model and the activations
        if self.transformer.wte.weight.device.type == "cuda":
            self.transformer.wte.to(dtype=torch.bfloat16)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            # https://arxiv.org/pdf/2310.17813
            fan_out = module.weight.size(0)
            fan_in = module.weight.size(1)
            std = 1.0 / math.sqrt(fan_in) * min(1.0, math.sqrt(fan_out / fan_in))
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=1.0)

    # TODO: bump base theta more, e.g. 100K is more common more recently
    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):
        # autodetect the device from model embeddings
        if device is None:
            device = self.transformer.wte.weight.device
        # stride the channels
        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)
        inv_freq = 1.0 / (base ** (channel_range / head_dim))
        # stride the time steps
        t = torch.arange(seq_len, dtype=torch.float32, device=device)
        # calculate the rotation frequencies at each (time, channel) pair
        freqs = torch.outer(t, inv_freq)
        cos, sin = freqs.cos(), freqs.sin()
        cos, sin = cos.bfloat16(), sin.bfloat16()  # keep them in bfloat16
        cos, sin = (
            cos[None, :, None, :],
            sin[None, :, None, :],
        )  # add batch and head dims for later broadcasting
        return cos, sin

    def get_device(self):
        return self.transformer.wte.weight.device

    def estimate_flops(self):
        """Return the estimated FLOPs per token for the model. Ref: https://arxiv.org/abs/2204.02311"""
        nparams = sum(p.numel() for p in self.parameters())
        nparams_embedding = self.transformer.wte.weight.numel()
        # For MoE, use active params instead of total
        if self.config.use_moe:
            total_expert_width = sum(
                count * size for count, size in self.config.expert_sizes
            )
            num_experts = sum(count for count, _ in self.config.expert_sizes)
            moe_params = (
                self.config.n_embd * total_expert_width * 2 * self.config.n_layer
            )
            inactive_moe = (
                moe_params
                * (num_experts - self.config.num_active_experts)
                // num_experts
            )
            nparams = nparams - inactive_moe
        l, h, q, t = (
            self.config.n_layer,
            self.config.n_head,
            self.config.n_embd // self.config.n_head,
            self.config.sequence_len,
        )
        num_flops_per_token = 6 * (nparams - nparams_embedding) + 12 * l * h * q * t
        return num_flops_per_token

    def setup_optimizers(
        self, unembedding_lr=0.004, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0
    ):
        model_dim = self.config.n_embd
        ddp, rank, local_rank, world_size = get_dist_info()
        # Separate out all parameters into 3 groups (matrix, embedding, lm_head)
        matrix_params = list(self.transformer.h.parameters())
        embedding_params = list(self.transformer.wte.parameters())
        lm_head_params = list(self.lm_head.parameters())
        assert len(list(self.parameters())) == len(matrix_params) + len(
            embedding_params
        ) + len(lm_head_params)
        # Create the AdamW optimizer for the embedding and lm_head
        # Scale the LR for the AdamW parameters by ∝1/√dmodel (having tuned the LRs for 768 dim model)
        dmodel_lr_scale = (model_dim / 768) ** -0.5
        if rank == 0:
            print(
                f"Scaling the LR for the AdamW parameters ∝1/√({model_dim}/768) = {dmodel_lr_scale:.6f}"
            )
        adam_groups = [
            dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),
            dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),
        ]
        adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)
        # Use sharded optimizers only if world_size is power of 2 (divisibility requirement)
        use_sharded = ddp and (world_size & (world_size - 1)) == 0
        AdamWFactory = (
            DistAdamW if use_sharded else partial(torch.optim.AdamW, fused=True)
        )
        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)
        # Create the Muon optimizer for the linear layers
        muon_kwargs = dict(lr=matrix_lr, momentum=0.95)
        MuonFactory = DistMuon if use_sharded else Muon
        muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)
        # Combine them the two optimizers into one list
        optimizers = [adamw_optimizer, muon_optimizer]
        for opt in optimizers:
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]
        return optimizers

    def forward(self, idx, targets=None, kv_cache=None, loss_reduction="mean"):
        B, T = idx.size()

        # Grab the rotary embeddings for the current sequence length (they are of shape (1, seq_len, 1, head_dim))
        assert T <= self.cos.size(1), (
            f"Sequence length grew beyond the rotary embeddings cache: {T} > {self.cos.size(1)}"
        )
        assert idx.device == self.cos.device, (
            f"Rotary embeddings and idx are on different devices: {idx.device} != {self.cos.device}"
        )
        assert self.cos.dtype == torch.bfloat16, "Rotary embeddings must be in bfloat16"
        # if kv cache exists, we need to offset the rotary embeddings to the current position in the cache
        T0 = 0 if kv_cache is None else kv_cache.get_pos()
        cos_sin = (
            self.cos[:, T0 : T0 + T],
            self.sin[:, T0 : T0 + T],
        )  # truncate cache to current sequence length

        # Forward the trunk of the Transformer
        x = self.transformer.wte(idx)
        x = norm(x)

        combined_aux_loss = None
        aux_loss_count = 0
        expert_usage_sum = None
        expert_usage_count = 0
        expert_usage_per_layer = []
        for block in self.transformer.h:
            x, aux_loss, f_i = block(x, cos_sin, kv_cache)

            if f_i is not None:
                if expert_usage_sum is None:
                    expert_usage_sum = f_i.clone()
                else:
                    expert_usage_sum += f_i
                expert_usage_count += 1
                expert_usage_per_layer.append(f_i.clone())

            if aux_loss is not None:
                if combined_aux_loss is None:
                    combined_aux_loss = {k: v.clone() for k, v in aux_loss.items()}
                else:
                    for key in aux_loss:
                        combined_aux_loss[key] += aux_loss[key]
                aux_loss_count += 1

        if combined_aux_loss is not None and aux_loss_count > 0:
            for key in combined_aux_loss:
                combined_aux_loss[key] /= aux_loss_count

        if expert_usage_sum is not None and expert_usage_count > 0:
            avg_expert_usage = expert_usage_sum / expert_usage_count
            if combined_aux_loss is None:
                combined_aux_loss = {}
            combined_aux_loss["expert_usage"] = avg_expert_usage
            combined_aux_loss["expert_usage_per_layer"] = expert_usage_per_layer

        x = norm(x)

        # Forward the lm_head (compute logits)
        softcap = 15
        if targets is not None:
            # training mode: compute and return the loss
            # TODO: experiment with Liger Kernels / chunked cross-entropy etc.
            logits = self.lm_head(x)
            logits = softcap * torch.tanh(logits / softcap)  # logits softcap
            logits = logits.float()  # use tf32/fp32 for logits
            ce_loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1,
                reduction=loss_reduction,
            )
            loss = ce_loss
            if combined_aux_loss is not None:
                loss = (
                    loss
                    + self.config.load_balance_loss_weight
                    * combined_aux_loss["load_balance_loss"]
                    + self.config.router_z_loss_weight
                    * combined_aux_loss["router_z_loss"]
                )
                if (
                    self.config.compute_loss_weight > 0
                    and "compute_loss" in combined_aux_loss
                ):
                    loss = (
                        loss
                        + self.config.compute_loss_weight
                        * combined_aux_loss["compute_loss"]
                    )
                combined_aux_loss["ce_loss"] = ce_loss

            return logits, loss, combined_aux_loss
        else:
            # inference mode: compute and return the logits
            logits = self.lm_head(x)
            logits = softcap * torch.tanh(logits / softcap)  # logits softcap
            return logits

    @torch.inference_mode()
    def generate(self, tokens, max_tokens, temperature=1.0, top_k=None, seed=42):
        """
        Naive autoregressive streaming inference.
        To make it super simple, let's assume:
        - batch size is 1
        - ids and the yielded tokens are simple Python lists and ints
        """
        assert isinstance(tokens, list)
        device = self.get_device()
        rng = None
        if temperature > 0:
            rng = torch.Generator(device=device)
            rng.manual_seed(seed)
        ids = torch.tensor([tokens], dtype=torch.long, device=device)  # add batch dim
        for _ in range(max_tokens):
            logits = self.forward(
                ids
            )  # (B, T, vocab_size) - inference returns just logits
            logits = logits[:, -1, :]  # (B, vocab_size)
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float("Inf")
            if temperature > 0:
                logits = logits / temperature
                probs = F.softmax(logits, dim=-1)
                next_ids = torch.multinomial(probs, num_samples=1, generator=rng)
            else:
                next_ids = torch.argmax(logits, dim=-1, keepdim=True)
            ids = torch.cat((ids, next_ids), dim=1)
            token = next_ids.item()
            yield token
